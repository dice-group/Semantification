{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervisied Semantification on FB15k-237"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantification process has four steps: (Data Preprocessing, KG Embedding, Clustering, and Entity Typing)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing:\n",
    "We skip data preprocessing step, since FB15k-237 dataset in already the knowledge graph format (RDF triples). This step is only required, if you have an input tabular data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KG Embedding:\n",
    "\n",
    "* In this step, we train a knowledge graph embedding (e.g., transE) to learn vector represenations of entities and their relations.\n",
    "* We use the graphVite embedding library to train transE on FB15k-237. For our experiments, we provide our pre-trained model in 'data/pre-trained'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH_TRANS_E = 'data/pre-trained/transE_fb15k_256dim.pkl'\n",
    "BASE_PATH_TRUTH = 'data/FB15k-237'\n",
    "\n",
    "# transe_fb15k-237.pkl: pre-trained model of fb15k.\n",
    "with open(PATH_TRANS_E, \"rb\") as fin:\n",
    "    model = pickle.load(fin)\n",
    "    \n",
    "entity2id = model.graph.entity2id\n",
    "relation2id = model.graph.relation2id\n",
    "\n",
    "entity_embeddings = model.solver.entity_embeddings\n",
    "relation_embeddings = model.solver.relation_embeddings\n",
    "\n",
    "entity_embeddings.shape\n",
    "\n",
    "#extract ground-truth types:\n",
    "fb_train=pd.read_csv(BASE_PATH_TRUTH + '/train.txt', sep='\\t', header=None, index_col=0)\n",
    "fb_valid=pd.read_csv(BASE_PATH_TRUTH + '/valid.txt', sep='\\t', header=None, index_col=0)\n",
    "fb_test=pd.read_csv(BASE_PATH_TRUTH + '/test.txt', sep='\\t', header=None, index_col=0)\n",
    "\n",
    "fb_df=pd.concat([fb_train, fb_valid, fb_test])\n",
    "fb_df['type']= fb_df[1].apply(lambda x: x.split('/')[1])\n",
    "\n",
    "#combine entities with their types:\n",
    "\n",
    "ground_truth={}\n",
    "for entity_id in entity2id.keys():\n",
    "    if entity_id in fb_df.index:\n",
    "        if isinstance(fb_df.loc[entity_id, 'type'], pd.core.series.Series): \n",
    "            ground_truth[entity_id]=fb_df.loc[entity_id, 'type'][0]\n",
    "        else:\n",
    "            ground_truth[entity_id]=fb_df.loc[entity_id, 'type']\n",
    "    else:\n",
    "        ground_truth[entity_id]='unknown' # for missed types"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#filter commen types from FB15k-237 dataset:\n",
    "entity_embedding_filter=[]\n",
    "y_true_filter=[]\n",
    "\n",
    "top_types=['people', 'film', 'location', 'music', 'soccer', 'education']\n",
    "\n",
    "for k, value in ground_truth.items():\n",
    "    if value in top_types:        \n",
    "        entity_embedding_filter.append(entity_embeddings[entity2id[k]])\n",
    "        y_true_filter.append(value)\n",
    "        \n",
    "X_all = np.asarray(entity_embedding_filter)\n",
    "\n",
    "#encode y_labels as one-hot:\n",
    "encoder = LabelEncoder()\n",
    "y_all = encoder.fit_transform(y_true_filter)\n",
    "labels = encoder.classes_.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering:\n",
    "* In this step, we group entities with similar properites (i.e., based on their embedding representations) into clusters. Each group should have similar entities --> similar types.\n",
    "\n",
    "* We employ a density-based clustering (hdbscan) to detect entities cluster based on their density in the embedding space.\n",
    "* We use the implementation of hdbscan clustering library. For more information/install, please check (https://hdbscan.readthedocs.io/en/latest/index.html "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "HDBSCAN requires two main hyper-parameters: 1) eplsion, which specify the area within it, there should be a min_samples to consider a point a core point. We use the eplow approach to find a best value for epslion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# final optimal value for cluster epsilon\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = neigh.fit(X_all)\n",
    "distances, indices = nbrs.kneighbors(X_all)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,-1]\n",
    "plt.plot(distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# compute the distance between entities using cosine\n",
    "X_all_double=X_all.astype(np.double)\n",
    "distance_matrix = pairwise_distances(X_all_double, metric='cosine')\n",
    "\n",
    "\n",
    "hdbscan_clusterer=hdbscan.HDBSCAN(algorithm='best', alpha=0.1, metric='precomputed', cluster_selection_method='leaf',\n",
    "                                      min_samples=10, min_cluster_size=700, core_dist_n_jobs=-1,allow_single_cluster=True,\n",
    "                                      cluster_selection_epsilon=0.9)\n",
    "\n",
    "\n",
    "\n",
    "hdbscan_clusterer.fit(distance_matrix)\n",
    "\n",
    "cluster_labels= hdbscan_clusterer.labels_\n",
    "cluster_probabilities=hdbscan_clusterer.probabilities_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entity Typing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sampling Entities for Labeling:\n",
    "In the following, we present our strategy to select entities based on its membership in l calsiter\n",
    "* We compute the cluster probabilies for all entities (cluster_probabilities). For each cluster, we select entities with high values >= 0.9 for labeling. \n",
    "* We present the selected entities (with their RDF triples) to human expers for labeling.\n",
    "* Finally, we propagate the most frequent type in each cluster to  all entities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### t-SNE Visualization of Labeled Entities:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# propagate the most frequent type in the cluster to all entities. \n",
    "df_tmp = pd.DataFrame({'pred_hdbscan': y_hdbscan, 'y_all': y_all})\n",
    "pred_hdbscan = df_tmp.groupby('pred_hdbscan').transform(lambda x: x.mode().iloc[0]).to_numpy().reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "X_2d = TSNE(random_state=42).fit_transform(X_all)\n",
    "label_ids = range(len(labels))\n",
    "colors=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']\n",
    "\n",
    "for i, c, label in zip(label_ids, colors, labels):    \n",
    "    plt.scatter(X_2d[pred_hdbscan == i, 0], X_2d[pred_hdbscan == i, 1], c=c, label=label, s=1)\n",
    "\n",
    "plt.legend()    \n",
    "plt.savefig('/src/Figures/fb15k-transE-hdbscan.png', dpi=600, bbox_inches='tight',pad_inches=0)    \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "os.chdir('../../')\n",
    "\n",
    "from clustering_evaluation import ClusterPurity\n",
    "evaluator=ClusterPurity()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy = accuracy_score(y_all, pred_hdbscan)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_all, pred_hdbscan, zero_division=0, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_all, pred_hdbscan, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_all, pred_hdbscan, average='weighted')\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "print ('Purity: ' , evaluator.purity_score(y_true=y_all, y_pred=pred_hdbscan))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}