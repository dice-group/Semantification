{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tab2Onto: Unsupervisied Semantification Of Lymphography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Data Preprocessing:\n",
    "In this step, we use [Vectograph Library](https://github.com/dice-group/vectograph) to convert lymphography data from tabular format into a knowledge graph (RDF triples)\n",
    "\n",
    "* For installation and usage of Vectograph, please follow the instructions described in https://github.com/dice-group/vectograph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py --tabularpath \"data/Lymphography/lympho.csv\" --kg_name \"lymphograph-KG.nt\" --num_quantile=10 --min_unique_val_per_column=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A preprocessed files of Lymphograpgy data can be found in `data/Lymphography/preprocessed/lymphograph-triples.nt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Knowledge Graph Embedding: \n",
    "* We used [DAIKIRI-Embedding library](https://github.com/dice-group/DAIKIRI-Embedding) to generate KG embeddings for Lymphography dataset\n",
    "* DAIKIRI-Embedding can be installed and used by following the instructions provided in https://github.com/dice-group/DAIKIRI-Embedding\n",
    "* A preprocessed file of lymphography embeddings can be found in `data/Lymphography/preprocessed/QMult_entity_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Clustering\n",
    "* We used K-means clustering provided into our package [DAIKIRI-Clustering](https://github.com/dice-group/DAIKIRI-Clustering)\n",
    "* For further details about the package installation, please check https://github.com/dice-group/DAIKIRI-Clustering\n",
    "* You can find a pre-processed file for Lymphography Clustering here `data/Lymphography/preprocessed/kmeans_Clusters`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lympho_df= pd.read_csv('./lymphograph-raw.csv', header=0, index_col=['patient'])\n",
    "y_true=lympho_df['class'].tolist()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_all = encoder.fit_transform(y_true)\n",
    "labels = encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df= pd.read_csv('./data/Lymphograph/QMult_entity_embeddings.csv', header=0,index_col=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, y_all, test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=103).fit(X_train)\n",
    "Kmeans_clusters= kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Entity Typing: (Human-In-the-Loop)\n",
    "* We developed [LabENT](https://github.com/dice-group/LabENT), a web application that incorporate human-in-the-loop to assign labels for the computed clusters. \n",
    "* We recommend users install and try LabENT. More details can be found https://github.com/dice-group/LabENT\n",
    "* LabENT Demo allows users to upload input files; labeled_clusters, and clustering_results to generate ontologies. \n",
    "* The generated ontology can be found in `data/Lymphography/preprocessed/DAIKIRI-Lympho.owl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Tab2Onto (Kmeans with ConEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation of Embedding-based Clustering (Kmeans, with ConEx embeddings) ###\n",
    "\n",
    "#----------- Evaluation based on Precision, Recall, Accuracy and F1-score: -------#\n",
    "accuracy = accuracy_score(y_test, Kmeans_clusters)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, Kmeans_clusters, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, Kmeans_clusters, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, Kmeans_clusters, average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accuracy: 0.666667`\n",
    "\n",
    "`Precision: 0.818182`\n",
    "\n",
    "`Recall: 0.666667`\n",
    "\n",
    "`F1 score: 0.728395`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Tab2Onto agains Supervised Baseline (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_clf = LogisticRegression(solver='liblinear',random_state=103).fit(X_train, y_train.ravel())\n",
    "y_lr = logistic_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Evaluation based on Precision, Recall, Accuracy and F1-score: -------#\n",
    "accuracy = accuracy_score(y_test, y_lr)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_lr, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, y_lr, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_lr, average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accuracy: 0.833333`\n",
    "\n",
    "`Precision: 0.814992`\n",
    "\n",
    "`Recall: 0.833333`\n",
    "\n",
    "`F1 score: 0.818254`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Tab2Onto against Random Labeling w.r.t Class Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# compute weights for classes according to their distribution:\n",
    "weights=[]\n",
    "y_counts=Counter(y_test)\n",
    "\n",
    "for i in range(4):\n",
    "    weights.append(y_counts[i]/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_random_bala=np.random.choice([0,1,2,3], size=y_test.shape[0], p=weights)\n",
    "\n",
    "# majority voting per cluster\n",
    "df_tmp = pd.DataFrame({'y_random': y_random_bala, 'y_test': y_test})\n",
    "y_random_bala = df_tmp.groupby('y_random').transform(lambda x: x.mode().iloc[0]).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Evaluation based on Precision, Recall, Accuracy and F1-score: -------#\n",
    "accuracy = accuracy_score(y_test, y_random_bala)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_random_bala, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, y_random_bala, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_random_bala, average='weighted')\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accuracy: 0.533333`\n",
    "\n",
    "`Precision: 0.487164`\n",
    "\n",
    "`Recall: 0.533333`\n",
    "\n",
    "`F1 score: 0.485556`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DAIKIRI)",
   "language": "python",
   "name": "daikiri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
