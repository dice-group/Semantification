{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tab2Onto: Unsupervisied Semantification Of FB15k-237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "os.makedirs('../results', exist_ok=True) \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "os.chdir('../../')\n",
    "\n",
    "from clustering_evaluation import ClusterPurity\n",
    "evaluator=ClusterPurity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Data Preprocessing:\n",
    "We skip the data preprocessing step, since FB15k-237 dataset in already linked data format (RDF triples). This step is only required, if you have an input tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def save_results(method, list_of_results): \n",
    "\n",
    "    out_file= open('../results/'+method+'_evaluation.txt', 'a')\n",
    "    out_file.write('\\n evaluation results of '+method+' (accuracy, precision, recall, f1, purity) on '+datetime.now()+'\\n')\n",
    "    for value in list_of_results:\n",
    "        out_file.write(value+'\\t')\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) KG Embedding:\n",
    "In this step, we train different KG embedding models, namely, transE, DistMult, rotatE, and ConEX to represent entities and their relations as low-dimensional semantic vectors. We use DAIKIRI-embedding libratry for computed KG models. For our experiments, we provide our pre-trained model in 'data/pre-trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRANS_E = 'data/pre-trained/transE_fb15k_256dim.pkl' \n",
    "BASE_PATH_TRUTH = 'data/FB15k-237'\n",
    "\n",
    "with open(PATH_TRANS_E, \"rb\") as fin:\n",
    "    model = pickle.load(fin)\n",
    "    \n",
    "entity2id = model.graph.entity2id\n",
    "relation2id = model.graph.relation2id\n",
    "\n",
    "entity_embeddings = model.solver.entity_embeddings\n",
    "relation_embeddings = model.solver.relation_embeddings\n",
    "\n",
    "entity_embeddings.shape\n",
    "\n",
    "#extract ground-truth types (e.g., film, person, organization, country, actor):\n",
    "fb_train=pd.read_csv(BASE_PATH_TRUTH + '/train.txt', sep='\\t', header=None, index_col=0)\n",
    "fb_valid=pd.read_csv(BASE_PATH_TRUTH + '/valid.txt', sep='\\t', header=None, index_col=0)\n",
    "fb_test=pd.read_csv(BASE_PATH_TRUTH + '/test.txt', sep='\\t', header=None, index_col=0)\n",
    "\n",
    "fb_df=pd.concat([fb_train, fb_valid, fb_test])\n",
    "fb_df['type']= fb_df[1].apply(lambda x: x.split('/')[1])\n",
    "\n",
    "#combine entities with their types:\n",
    "ground_truth={}\n",
    "for entity_id in entity2id.keys():\n",
    "    if entity_id in fb_df.index:\n",
    "        if isinstance(fb_df.loc[entity_id, 'type'], pd.core.series.Series): \n",
    "            ground_truth[entity_id]=fb_df.loc[entity_id, 'type'][0]\n",
    "        else:\n",
    "            ground_truth[entity_id]=fb_df.loc[entity_id, 'type']\n",
    "    else:\n",
    "        ground_truth[entity_id]='unknown' # for missed types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter top-k types from FB15k-237 dataset, we set k=6 in our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter commen types from FB15k-237 dataset:\n",
    "entity_embedding_filter=[]\n",
    "y_true_filter=[]\n",
    "\n",
    "top_types=['people', 'film', 'location', 'music', 'soccer', 'education']\n",
    "\n",
    "for k, value in ground_truth.items():\n",
    "    if value in top_types:        \n",
    "        entity_embedding_filter.append(entity_embeddings[entity2id[k]])\n",
    "        y_true_filter.append(value)\n",
    "        \n",
    "X_all = np.asarray(entity_embedding_filter)\n",
    "\n",
    "#encode y_labels as one-hot:\n",
    "encoder = LabelEncoder()\n",
    "y_all = encoder.fit_transform(y_true_filter)\n",
    "labels = encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Entities Clustering:\n",
    "* In this step, we group entities with similar properites (i.e., based on their embedding representations) into clusters. Each group should have similar entities --> similar types.\n",
    "\n",
    "* We employ different clustering approachs (k-means, hdbscan, agglomerative) for our experiments to find which method yields best in entity typing task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDBSCAN requires two main hyper-parameters: 1) eplsion, which specify the area within it, there should be a min_samples to consider a point a core point. We use the eplow approach to find a best value for epslion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# final optimal value for cluster epsilon\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = neigh.fit(X_all)\n",
    "distances, indices = nbrs.kneighbors(X_all)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,-1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# compute the distance between entities using cosine\n",
    "X_all_double=X_all.astype(np.double)\n",
    "distance_matrix = pairwise_distances(X_all_double, metric='cosine')\n",
    "\n",
    "hdbscan_clusterer=hdbscan.HDBSCAN(algorithm='best', alpha=0.1, metric='precomputed', cluster_selection_method='leaf',\n",
    "                                      min_samples=10, min_cluster_size=700, core_dist_n_jobs=-1,allow_single_cluster=True,\n",
    "                                      cluster_selection_epsilon=0.9)\n",
    "\n",
    "hdbscan_clusterer.fit(distance_matrix)\n",
    "\n",
    "cluster_labels= hdbscan_clusterer.labels_\n",
    "cluster_probabilities=hdbscan_clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity Typing: \n",
    "\n",
    "In the following, we present our strategy to select entities based on its distance to the cluster centroid\n",
    "* We compute the cluster probabilies for all entities (cluster_probabilities). For each cluster, we select entities with high values >= 0.9 for labeling. \n",
    "* We present the selected entities (with their RDF triples) to human expers for labeling.\n",
    "* Finally, we propagate the most frequent type in each cluster to  all entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagate the most frequent type in the cluster to all entities. \n",
    "df_tmp = pd.DataFrame({'pred_hdbscan': y_hdbscan, 'y_all': y_all})\n",
    "pred_hdbscan = df_tmp.groupby('pred_hdbscan').transform(lambda x: x.mode().iloc[0]).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization of entity typing.\n",
    "plt.figure(figsize=(6, 5))\n",
    "X_2d = TSNE(random_state=42).fit_transform(X_all)\n",
    "label_ids = range(len(labels))\n",
    "colors=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']\n",
    "\n",
    "for i, c, label in zip(label_ids, colors, labels):    \n",
    "    plt.scatter(X_2d[pred_hdbscan == i, 0], X_2d[pred_hdbscan == i, 1], c=c, label=label, s=1)\n",
    "\n",
    "plt.legend()    \n",
    "plt.savefig('/src/Figures/fb15k-transE-hdbscan.png', dpi=600, bbox_inches='tight',pad_inches=0)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation of HDBSCAN Clustering: \n",
    "\n",
    "accuracy = accuracy_score(y_all, pred_hdbscan)\n",
    "precision = precision_score(y_all, pred_hdbscan, zero_division=0, average='weighted')\n",
    "recall = recall_score(y_all, pred_hdbscan, average='weighted')\n",
    "f1 = f1_score(y_all, pred_hdbscan, average='weighted')\n",
    "purity= evaluator.purity_score(y_true=y_all, y_pred=pred_hdbscan)\n",
    "\n",
    "# save results into file\n",
    "save_results(method='hdbscan', list_of_results=[accuracy, precision, recall, f1, purity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%%time\n",
    "kmeans = KMeans(n_clusters=len(top_types), random_state=35).fit(X_all)\n",
    "pred_kmeans = kmeans.predict(X_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity Typing: \n",
    "- Similar to hdbscan, we repeat the same evaluation of entity typing for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting per cluster\n",
    "df_tmp = pd.DataFrame({'pred_Kmeans': pred_kmeans, 'y_all': y_all})\n",
    "pred_kmeans = df_tmp.groupby('pred_Kmeans').transform(lambda x: x.mode().iloc[0]).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation of K-means Clustering: \n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, c, label in zip(label_ids, colors, labels):    \n",
    "    plt.scatter(X_2d[pred_kmeans == i, 0], X_2d[pred_kmeans == i, 1], c=c, label=label, s=1)\n",
    "\n",
    "plt.legend()    \n",
    "plt.savefig('/src/Figures/fb15k-transE-Kmeans.png', dpi=600, bbox_inches='tight',pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation of Kmeans Clustering: \n",
    "accuracy = accuracy_score(y_all, pred_kmeans)\n",
    "precision = precision_score(y_all, pred_kmeans, zero_division=0, average='weighted')\n",
    "recall = recall_score(y_all, pred_kmeans, average='weighted')\n",
    "f1 = f1_score(y_all, pred_kmeans, average='weighted')\n",
    "purity= evaluator.purity_score(y_true=y_all, y_pred=pred_kmeans)\n",
    "\n",
    "# save results into file\n",
    "save_results(method='kmeans', list_of_results=[accuracy, precision, recall, f1, purity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "%%time\n",
    "aggClustering = AgglomerativeClustering(n_clusters=len(top_types))\n",
    "y_aggClustering=aggClustering.fit_predict(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity Typing: \n",
    "- Similar to hdbscan, we repeat the same evaluation of entity typing for Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting per cluster\n",
    "df_tmp = pd.DataFrame({'pred_agglom': y_aggClustering, 'y_all': y_all})\n",
    "pred_agglom = df_tmp.groupby('pred_agglom').transform(lambda x: x.mode().iloc[0]).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation of K-means Clustering: \n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, c, label in zip(label_ids, colors, labels):    \n",
    "    plt.scatter(X_2d[pred_kmeans == i, 0], X_2d[pred_agglom == i, 1], c=c, label=label, s=1)\n",
    "\n",
    "plt.legend()    \n",
    "plt.savefig('/src/Figures/fb15k-transE-Agglomerative.png', dpi=600, bbox_inches='tight',pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation of Agglomerative Clustering: \n",
    "accuracy = accuracy_score(y_all, pred_agglom)\n",
    "precision = precision_score(y_all, pred_agglom, zero_division=0, average='weighted')\n",
    "recall = recall_score(y_all, pred_agglom, average='weighted')\n",
    "f1 = f1_score(y_all, pred_agglom, average='weighted')\n",
    "purity= evaluator.purity_score(y_true=y_all, y_pred=pred_agglom)\n",
    "\n",
    "# save results into file\n",
    "save_results(method='Agglomerative', list_of_results=[accuracy, precision, recall, f1, purity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
